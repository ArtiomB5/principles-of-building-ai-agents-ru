# 19
## НАСТРОЙКА ВАШЕГО RAG-ПАЙПЛАЙНА

### **Разбиение на фрагменты (Chunking)**

Разбиение на фрагменты — это процесс разделения больших документов на меньшие, управляемые части для обработки.

Ключевое, что вам нужно выбрать здесь, — это стратегия и окно перекрытия (overlap window). Хорошее разбиение на фрагменты обеспечивает баланс между сохранением контекста и степенью детализации поиска.

Стратегии разбиения включают рекурсивное, символьное, учитывающее токены и специфичное для формата (Markdown, HTML, JSON, LaTeX) разделение. Mastra поддерживает все из них.

### **Векторное представление (Embedding)**

Эмбеддинги — это числовые представления текста, которые отражают его семантическое значение. Эти векторы позволяют нам выполнять поиск по сходству. Mastra поддерживает нескольких поставщиков эмбеддингов, таких как OpenAI и Cohere, с возможностью генерировать эмбеддинги как для отдельных фрагментов, так и для массивов текста.

### **Upsert**

Операции Upsert позволяют вставлять или обновлять векторы и связанные с ними метаданные в вашем векторном хранилище. Эта операция необходима для поддержания вашей базы знаний, объединяя как векторы внедрения, так и любые дополнительные метаданные, которые могут быть полезны для поиска.

### **Индексация (Indexing)**

Индекс — это структура данных, которая оптимизирует поиск векторного сходства. При создании индекса вы указываете параметры, такие как размерность (соответствующая вашей модели эмбеддингов) и метрика сходства (косинусная, евклидова, скалярное произведение). Это одноразовый шаг настройки для каждой коллекции векторов.

### **Запросы (Querying)**

Запросы включают преобразование пользовательского ввода в эмбеддинг и поиск похожих векторов в вашем векторном хранилище. Базовый запрос возвращает наиболее семантически похожие фрагменты на ваш ввод, обычно с оценкой сходства. *Под капотом* это куча матричных умножений, чтобы найти ближайшую точку в *n-*мерном пространстве (подумайте о геопоиске с широтой/долготой, но в 1536 измерениях).

Самый распространенный алгоритм, который это делает, называется косинусное сходство (хотя вы можете использовать и другие).

*   **Гибридные запросы с метаданными.** Гибридные запросы объединяют поиск векторного сходства с традиционной фильтрацией по метаданным. Это позволяет вам сужать результаты на основе как семантического сходства, так и структурированных полей метаданных, таких как даты, категории или пользовательские атрибуты.

### **Переранжирование (Reranking)**

Переранжирование — это этап последующей обработки, который повышает актуальность результатов за счет применения более сложных методов оценки. Он учитывает такие факторы, как семантическая релевантность, векторное сходство и смещение позиции, чтобы переупорядочить результаты для лучшей точности.

Это более трудоемкий с точки зрения вычислений процесс, поэтому вы, как правило, не хотите запускать его по всему корпусу из—за задержки - обычно вы просто запускаете его на примере кода.

### **Пример кода**

Вот некоторый код, использующий Mastra для настройки конвейера RAG. Mastra включает в себя согласованный интерфейс для создания индексов, обновления вложений и запросов, предлагая при этом свои собственные уникальные функции и оптимизацию, поэтому, хотя в этом примере используется Pinecone, вы можете легко использовать другую базу данных.

```javascript
import { Mastra } from "@mastra/core";
import { MDocument, PyVector } from "@mastra/rag";
import { embedMany, embed } from "ai";
import { openai } from "@ai-sdk/openai";

// Initialize document and create chunks
const doc = MDocument.fromText('Your text content here...');
const chunks = await doc.chunk({
    strategy: "recursive",
    size: 512,
    overlap: 50,
});

// Generate embeddings
const { embeddings } = await embedMany({
    values: chunks.map(chunk => chunk.text),
    model: openai.embedding("text-embedding-3-small"),
});

// Initialize vector store and Mastra
const pyVector = new PyVector(process.env.POSTGRES_CONNECTION_STRING!);
const mastra = new Mastra({ vectors: { pyVector } });

// Store embeddings
await pyVector.createIndex("embeddings", 1536);
await pyVector.upsert(
    "embeddings",
    embeddings,
    chunks?.map(chunk => ({ text: chunk.text }))
);

// Query example
const query = "insert query here";
const { embedding } = await embed({
    value: query,
    model: openai.embedding("text-embedding-3-small"),
});

// Retrieve similar chunks
const results = await pyVector.query("embeddings", embedding);
const relevantContext = results
    .map(result => result?.metadata?.text)
    .join("\n\n");

// Generate response
const completion = await openai("gpt-4o-mini").generate(
    `Please answer the following question:
    ${query}

    Based on this context: ${relevantContext}
    If the context lacks sufficient information, please state that
    explicitly.`
);

console.log(completion.text);
```

Примечание: Существуют продвинутые способы выполнения RAG: использование LLMS для генерации метаданных, использование LLMS для уточнения поисковых запросов; использование 82 графических баз данных SAM BHAGWAT для моделирования сложных взаимосвязей. Это может быть полезно для вас, но сначала начните с настройки рабочего пайплаина и настройки обычных параметров — моделей встраивания, повторных рейтингов, алгоритмов разбивки на блоки.
