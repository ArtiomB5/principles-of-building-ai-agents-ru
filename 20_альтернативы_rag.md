# 20
## АЛЬТЕРНАТИВЫ RAG

Отлично, теперь вы знаете, как работает RAG.
Или, выражаясь как владелец твиттера, RAG мертв?

Еще нет, мы думаем. Но есть некоторые более простые подходы, к которым вам, вероятно, следует обратиться в первую очередь, прежде чем настраивать RAG-пайплайн.

### **Агентный RAG**

Вместо поиска по документам вы можете дать вашему агенту набор инструментов, чтобы помочь ему рассуждать о домене. Например, агент-финансовый консультант может иметь доступ к API рыночных данных, калькуляторам и инструментам анализа портфеля. Затем агент может использовать эти инструменты для генерации более точных и обоснованных ответов.

Преимущество агентного RAG заключается в том, что он может быть более точным, чем RAG — вместо поиска релевантного текста агент может вычислять точные ответы.

Недостаток в том, что вам нужно создавать и поддерживать инструменты, и агент должен знать, как эффективно их использовать.

Одна из наших инвесторов построила различные инструменты для запросов к своему веб-сайту разными способами, а затем объединила их в MCP-сервер, который она могла дать агенту Windsurf.

Затем она записала демо, где она спросила агента о своих любимых ресторанах (он порекомендовал Flour + Water в Сан-Франциско) и своих любимых портфельных компаниях (он уклонился, сказав, что она любит все свои компании одинаково). ∗

### **Генерация с усилением рассуждения (ReAG)**

ReAG — это свободная группа идей, которая фокусируется на улучшении использования моделей для обогащения текстовых фрагментов.

Сторонники ReAG говорят, что вы должны подумать о том, что бы вы сделали с бюджетом на LLM в 10 раз больше, чтобы улучшить качество вашего RAG-пайплайна, — а затем сделать это.

Они указывают, что предварительная обработка является асинхронной, поэтому ей не нужно быть быстрой.

Некоторые мыслительные эксперименты для рассмотрения, если вы думаете о ReAG:

*   когда вы аннотируете, отправляйте запрос к модели в 10 раз больше при высокой температуре, чтобы увидеть, есть ли консенсус в ответах.
*   отправляйте входные данные через LLM перед извлечением данных
*   извлекайте богатую семантическую информацию, включая ссылки на другие разделы, имена сущностей и любые структурированные отношения

### **Полная загрузка контекста**

С новыми моделями, поддерживающими большие контекстные окна (у Gemini 2 млн токенов), иногда самый простой подход — просто загрузить весь релевантный контент напрямую в контекст. Это работает довольно хорошо с моделями, оптимизированными для рассуждений над длинными контекстами, такими как Claude или GPT-4.

Преимущества — простота и надежность — не нужно беспокоиться о разбиении на фрагменты или извлечении, и модель может видеть весь контекст сразу. Основные ограничения:

*   Стоимость (вы платите за все контекстное окно)
*   Ограничения по размеру (даже большие окна имеют пределы)
*   Потенциал для отвлечения (модель может сосредоточиться на нерелевантных частях)

### **Заключение**

Мы инженеры. А инженеры могут усложнять вещи.

С RAG вы должны бороться с этой тенденцией. Начните с простого, проверьте качество, усложняйте.

*   **Шаг первый**, вы должны закидывать весь ваш корпус в контекстное окно Gemini.
*   **Шаг второй**, напишите кучу функций для доступа к вашему набору данных, объедините их в MCP-сервер и дайте их агенту Cursor или Windsurf.

Если ни первый, ни второй шаг не дают вам достаточно хорошего качества, тогда рассмотрите возможность построения RAG-пайплайна.

∗
Код доступен по адресу https://github.com/alanagoyal/mcp-server